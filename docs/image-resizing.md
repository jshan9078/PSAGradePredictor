# Image Resizing for PSA Card Grading

## The Challenge

Pokemon cards come in varying image sizes from scans:
- Original: ~1200-1300 x 900 pixels
- Aspect ratio: ~1.4:1 (taller than wide)
- **Problem**: PyTorch DataLoader requires all images in a batch to be the same size

---

## âŒ **Bad Approach: Naive Resize (Distortion)**

```python
# Simple resize to 224x224
cv2.resize(image, (224, 224))
```

### What This Does:
```
Original:  1257 x 902  (aspect ratio 1.39:1)
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚          â”‚
           â”‚   CARD   â”‚
           â”‚          â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Naive Resize: 224 x 224  (aspect ratio 1:1)
           â”Œâ”€â”€â”€â”€â”€â”
           â”‚CARD â”‚  â† Squished horizontally!
           â””â”€â”€â”€â”€â”€â”˜
```

### What Gets Lost:
- âŒ **Aspect ratio distortion** - Cards look wider than they are
- âŒ **Edge features compressed** - Corner wear hard to see
- âŒ **Fine details lost** - Small scratches disappear

---

## âœ… **Good Approach: Aspect-Ratio Preserving Resize (Padding)**

```python
# Resize maintaining aspect ratio, then pad
resize_with_aspect_ratio(image, target_size=224, pad_value=0)
```

### What This Does:
```
Original:  1257 x 902
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚          â”‚
           â”‚   CARD   â”‚
           â”‚          â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: Resize to fit
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚        â”‚
           â”‚  CARD  â”‚  â† Scaled down proportionally
           â”‚        â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           224 x 161

Step 2: Pad to square
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  (black)   â”‚  â† Padding (top)
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚            â”‚
      â”‚    CARD    â”‚  â† Actual card (preserved!)
      â”‚            â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚  (black)   â”‚  â† Padding (bottom)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      224 x 224
```

### What's Preserved:
- âœ… **Aspect ratio maintained** - Card looks natural
- âœ… **All edge details** - Corners, wear, whitening visible
- âœ… **Fine features** - Scratches, print dots preserved
- âœ… **No distortion** - Card is exactly as photographed

---

## ğŸ” **Real-World Impact**

### Example: Detecting Corner Wear

**With Distortion (Bad)**:
```
Original Corner:     Distorted Corner:
  â”Œâ”€â”                  â”Œâ”
  â”‚ â”‚                  â”‚â”‚  â† Compressed!
  â”‚ â”‚ whitening        â”‚â”‚  whitening hard to see
  â””â”€â”˜                  â””â”˜
```

**With Padding (Good)**:
```
Original Corner:     Padded Corner:
  â”Œâ”€â”                  â”Œâ”€â”
  â”‚ â”‚                  â”‚ â”‚  â† Same proportions!
  â”‚ â”‚ whitening        â”‚ â”‚  whitening clearly visible
  â””â”€â”˜                  â””â”€â”˜
```

---

## ğŸ“Š **Image Size Trade-offs**

### Current: 224x224
- âœ… Fast training (~30 min/epoch on T4 GPU)
- âœ… Works with pretrained ResNet weights
- âš ï¸ Some fine detail loss vs original

### Alternative: 384x384
```python
# In train.py or dataset init
PSADataset(manifest, image_size=(384, 384))
```

**Pros**:
- âœ… More detail preserved (70% more pixels)
- âœ… Better for tiny defects

**Cons**:
- âŒ Slower training (~3x longer)
- âŒ More memory (may need smaller batch size)

### Alternative: 512x512 (Maximum Detail)
```python
PSADataset(manifest, image_size=(512, 512))
```

**Pros**:
- âœ… Maximum detail (5x more pixels than 224)
- âœ… Best for detecting subtle flaws

**Cons**:
- âŒ Much slower (~5-6x)
- âŒ High memory (batch_size may drop to 8-16)
- âŒ Requires more training data to avoid overfitting

---

## ğŸ¯ **Recommendation**

### For Initial Training: **224x224** (Current)
- Fast iteration
- Good balance of speed and quality
- Standard ResNet size

### After Validation: **Test 384x384**
If model performance plateaus, try larger images:
```python
# In train.py
train_dataset = PSADataset(
    splits['train'],
    image_size=(384, 384)  # More detail
)
```

---

## ğŸ§ª **Testing Different Sizes**

You can experiment by running training with different sizes:

```bash
# Small - fast testing (224x224)
python src/train.py --image_size 224

# Medium - better detail (384x384)
python src/train.py --image_size 384

# Large - maximum quality (512x512)
python src/train.py --image_size 512
```

**Note**: Larger images need smaller batch sizes:
- 224x224: batch_size=32 âœ…
- 384x384: batch_size=16
- 512x512: batch_size=8

---

## ğŸ“ˆ **Expected Performance Impact**

| Size | Training Time | Memory | Detail Preserved | QWK Expected |
|------|---------------|--------|------------------|--------------|
| 224x224 | 30 min/epoch | 4 GB | Good | 0.85-0.88 |
| 384x384 | 90 min/epoch | 8 GB | Better | 0.87-0.90 |
| 512x512 | 150 min/epoch | 12 GB | Best | 0.88-0.91 |

**Diminishing returns**: Going from 224â†’384 helps more than 384â†’512.

---

## ğŸš€ **Current Implementation**

Your dataset now uses **aspect-ratio preserving resize with padding**:

```python
# In src/dataset.py
front_6ch = resize_with_aspect_ratio(
    front_6ch,
    target_size=224,  # Default
    pad_value=0       # Black padding
)
```

This ensures:
1. âœ… No distortion - cards maintain natural proportions
2. âœ… All features preserved - edges, corners, surface details
3. âœ… Real-world compatibility - works with any card photo
4. âœ… Batch-able - all images are same size (224x224)

---

## ğŸ“ **Bottom Line**

**Your model will work with real-world photos** because:
- Aspect ratio is preserved (no distortion)
- All edge/corner details are kept (no compression)
- Padding is black (neutral, doesn't confuse model)
- Size is configurable (can increase later if needed)

The 224x224 default is a great starting point - it's what ResNet was designed for and provides good quality while training fast. If you need more detail later, just increase to 384x384 and retrain! ğŸš€
