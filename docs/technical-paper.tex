\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,graphicx,booktabs,geometry}
\usepackage{hyperref}
\geometry{margin=1in}

\title{Designing a Reliable PSA Grade Predictor:\\
A Task-Tuned Dual-Branch Residual CNN with Ordinal, Imbalance-Aware Learning}
\author{}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We present a task-specific architecture for predicting the PSA grade (1--10) of a Pokémon card given paired scans of its front and back. 
The design marries classical convolutional inductive biases with lightweight attention and ordinal, imbalance-aware objectives. 
A dual-branch asymmetric ResNet processes front and back separately, emphasizing the back---where edging and whitening are most diagnostic---then fuses features for a calibrated probability distribution over grades. 
We motivate each component from both practical grading heuristics and mathematical principles (losses, activations, invariances), and we describe the full data path: geometric normalization, augmentation, model, losses, and dataset splits.
\end{abstract}

\section{Problem Statement and Label Geometry}
Given paired RGB images $x^{\text{front}}$ and $x^{\text{back}}$, the task is to infer a categorical distribution
\[
p(y=k \mid x^{\text{front}},x^{\text{back}}), \qquad k \in \{1,\dots,10\},
\]
where labels reflect ordered quality levels. 
This ordering implies a natural metric on labels $|k-\ell|$, so mispredicting 9 for 10 is less severe than 9 for 3. 
We exploit this geometry in both the loss and evaluation.

\subsection*{Class Imbalance}
The dataset of $N = 10{,}906$ samples is heavily skewed toward higher grades:

\begin{center}
\begin{tabular}{lrrrrrrrrrr}
\toprule
Grade & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
Count & 57 & 26 & 45 & 80 & 232 & 573 & 986 & 2222 & 3903 & 2782 \\
\bottomrule
\end{tabular}
\end{center}

To prevent the objective from being dominated by grades 9--10, we employ a \textbf{class-weighted classification loss} together with an \textbf{ordinal consistency loss}. 
Let $x_i$ denote the $i$th input (front/back scan pair), $y_i \in \{1,\dots,10\}$ its true grade, and $P_\theta(y \mid x_i)$ the model’s predicted probability (softmax) under parameters $\theta$.

\paragraph{Component losses.}
\begin{align}
L_{\text{weighted}}(\theta) &= 
\frac{1}{N}\sum_{i=1}^{N} 
w_{y_i}\,\big(-\log P_\theta(y_i \mid x_i)\big), \\
L_{\text{ord}}(\theta) &= 
\frac{1}{N}\sum_{i=1}^{N} 
\big(\hat{y}_i - y_i\big)^2,
\qquad
\hat{y}_i = \sum_{k=1}^{10} k\,P_\theta(y=k\mid x_i).
\end{align}

\noindent\textbf{Total loss.}
\begin{equation}
L_{\text{total}}(\theta) = 
L_{\text{weighted}}(\theta)
+ \lambda\,L_{\text{ord}}(\theta)
\end{equation}

\paragraph{Definitions.}
\begin{itemize}
  \item $w_{y_i}$ --- class weight for label $y_i$; we set $w_k = \dfrac{N}{n_k}$ (optionally normalized) where $n_k$ is the count of grade $k$. This balances per-class influence despite the skewed distribution.
  \item $\hat{y}_i$ --- expected (mean) grade under the model’s posterior, encouraging nearby grades to be closer than far-apart ones.
  \item $\lambda > 0$ --- trade-off between classification accuracy (via $L_{\text{weighted}}$) and ordinal smoothness (via $L_{\text{ord}}$).
\end{itemize}

\paragraph{Choice of $\lambda$.}
We performed a logarithmic sweep $\lambda \in \{10^{-3},10^{-2},10^{-1},1,10\}$ on a held-out validation set and selected $\lambda^\ast$ to (i) minimize validation grade MAE / maximize quadratic-weighted kappa, and (ii) yield comparable mid-training magnitudes for the two loss components:
\[
\mathbb{E}\!\left[L_{\text{weighted}}\right]
\approx
\mathbb{E}\!\left[\lambda^\ast L_{\text{ord}}\right].
\]
This procedure yielded $\lambda^\ast = 0.3$, which preserved minority-class influence while enforcing a smooth, ordered decision boundary across adjacent grades.


\section{Input Standardization}

\subsection{Photometric Stabilization}
Although all scans originate from the same scanner under consistent illumination, subtle local contrast variations still occur across card surfaces due to reflections, glare, or texture.
To correct these effects without distorting the card’s true coloration, we perform illumination equalization in the LAB color space rather than RGB.

Unlike RGB, where brightness and color information are intertwined across the three channels, LAB separates lightness (L) from chromatic components (a and b).
This separation allows us to adjust brightness and contrast independently of color.
Specifically, we apply contrast-limited adaptive histogram equalization (CLAHE) only to the L channel to locally normalize exposure and reduce glare, while leaving the a and b channels untouched to preserve authentic color characteristics such as ink tone, holo hue, and surface tint.

This approach produces images with uniform lighting yet faithful color representation—ensuring that visual cues critical to grading accuracy, like centering tones and surface coloration, remain consistent and reliable.

\subsection{Auxiliary Gradient Channels}
Whitening, surface wear, and micro-scratches appear primarily as high-frequency luminance variations concentrated along card borders and reflective regions. 
Although the LAB color space already separates luminance ($L$) from chromaticity ($a,b$), a conventional convolutional network must still learn spatial derivatives internally to become sensitive to such high-frequency cues. 
To make this information explicit, we augment the input with derivative-based channels computed directly from the stabilized $L$ map.

Given the LAB-stabilized image
\[
x = [\,L,\,a,\,b\,],
\]
we compute first- and second-order spatial derivatives of the lightness channel:
\[
G_x = \partial_x L, \qquad 
G_y = \partial_y L, \qquad 
\Delta L = \partial_{xx}L + \partial_{yy}L.
\]
Here, $G_x$ captures horizontal brightness transitions (vertical edges), $G_y$ captures vertical transitions (horizontal edges), and $\Delta L$---the Laplacian---highlights points of strong curvature or isotropic intensity change such as corners, specks, or fine scratches.  
Together, these channels form a compact description of local structure:
\begin{itemize}
    \item \textbf{$G_x$ (horizontal gradient)} --- sensitive to vertical edge wear, whitening streaks, and print-boundary erosion along card sides.
    \item \textbf{$G_y$ (vertical gradient)} --- emphasizes horizontal surface features such as top and bottom edge chipping or linear scratches.
    \item \textbf{$\Delta L$ (Laplacian)} --- accentuates high-curvature intensity variations, capturing small reflective defects, corner rounding, and micro-texture irregularities.
\end{itemize}

The three derivative maps are concatenated with the base LAB channels to form a six-channel tensor:
\[
I_{\text{input}} = [\,L,\,a,\,b,\,G_x,\,G_y,\,\Delta L\,].
\]
This representation provides early convolutional layers with direct access to both color information and explicit measures of edge magnitude, orientation, and curvature. 
By supplying these structural features a priori, the network need not relearn fundamental edge-detection kernels, improving convergence efficiency and enhancing sensitivity to whitening, texture damage, and fine surface imperfections that strongly influence grading outcomes.




\section{Architecture: Asymmetric Dual Encoders with Edge- and Centering-Aware Attention}

\subsection{Motivation for Two Encoders}
Human graders typically assess the back first for whitening and surface wear, and then the front for centering, gloss, and scratches. 
The front designs vary greatly across sets and holo patterns, whereas the back has a consistent layout. 
To mirror this asymmetry, the architecture allocates greater representational capacity to the back encoder, responsible for precise defect detection and border geometry, while using a lighter front encoder that learns appearance invariances.

\begin{itemize}
    \item Back branch $f_{\theta_b}$: ResNet-34 (deeper, detailed texture extraction).
    \item Front branch $f_{\theta_f}$: ResNet-18 (shallower, color and artwork invariance).
\end{itemize}
Let the resulting embeddings be $h_b \in \mathbb{R}^{d_b}$ and $h_f \in \mathbb{R}^{d_f}$.

\subsection{Residual Learning (Task Rationale)}
\label{subsec:residual-learning}

We use residual blocks in both encoders so that deeper models remain stable while preserving fine edge evidence critical for grading.
A residual block computes
\[
h^{\ell+1} \;=\; h^\ell \;+\; \mathcal{F}\!\left(h^\ell; W_\ell\right),
\]
where $h^\ell$ is the input feature map, $\mathcal{F}$ is a small Conv–Norm–Activation stack, and $W_\ell$ are its parameters.

\paragraph{Why this matters for grading.}
\begin{itemize}
  \item \textbf{Protects micro-defects:} The identity path ($h^\ell$) carries forward already-detected edge whitening and hairline scratches, while $\mathcal{F}$ adds refinements. This avoids ``washing out'' weak but important signals across many layers.
  \item \textbf{Supports multi-scale cues:} Depth is needed to combine local textures (corner chips) with global layout (centering). Residual addition lets us go deeper on the back branch (ResNet-34) without destabilizing training.
  \item \textbf{Stable fine-tuning on limited data:} With $\approx 10$k scans, residual encoders transfer well from ImageNet and adapt reliably, reducing the risk of overfitting to artwork idiosyncrasies on the front.
  \item \textbf{Better attention synergy:} CBAM on the back benefits from strong lower-level edges that persist through the skip; attention can then highlight borders and corners instead of relearning edges every stage.
  \item \textbf{Centricity awareness:} Deeper residual stacks aggregate spatial evidence across the whole frame, improving sensitivity to left/right and top/bottom border asymmetries without aggressive geometric preprocessing.
\end{itemize}

\paragraph{Notation (brief).}
$h^\ell$: block input features; $\mathcal{F}(\cdot;W_\ell)$: residual transformation; $W_\ell$: block parameters; $L$: our training loss (weighted CE + EMD + optional auxiliaries).


\subsection{Lightweight Attention on the Back Branch}
\label{subsec:attention-back}

The back of a Pokémon card is the most standardized and diagnostically rich side for grading: 
all cards share the same design, so any visual deviation—such as whitening, edge chipping, or surface scratches—is an indicator of wear. 
To help the network focus on these subtle yet spatially structured cues, we attach a lightweight attention module (CBAM) to the back encoder only.

\paragraph{Purpose.}
While a plain convolution processes every pixel equally, most of the back surface contains uniform blue regions that carry little information. 
Attention reweights features so that high-value regions (edges, corners, and scratch lines) contribute more to the embedding than visually redundant areas. 
This directs representational capacity toward the areas human graders examine first.

\paragraph{Mechanism overview.}
Given feature maps $F \in \mathbb{R}^{C \times H \times W}$ from a residual block, 
the module produces two complementary attention masks:
\begin{enumerate}
    \item \textbf{Channel attention} learns which feature channels are most relevant.  
    It aggregates spatial information via global average and max pooling, forming a descriptor that measures how strongly each channel responds to key patterns such as ``edge texture,'' ``glare,'' or ``surface noise.''  
    A small gating network then generates channel weights $\alpha \in [0,1]^C$, and the reweighted output $\alpha \odot F$ amplifies channels tied to grading defects.
    \item \textbf{Spatial attention} learns \emph{where} in the image these features matter most.  
    By pooling across channels and convolving the result, it yields a 2-D mask $M_s \in [0,1]^{H \times W}$ emphasizing border zones and corners.  
    The modulated features $F' = M_s \odot (\alpha \odot F)$ strengthen spatial focus on whitening and centering boundaries.
\end{enumerate}

\paragraph{Domain-specific bias.}
To encode prior knowledge that edges are critical, we append a simple binary rim mask $B$ (1 on the outer 5--8\% of pixels, 0 elsewhere) to the feature tensor before attention. 
This teaches the network that information near the card border should be prioritized even before learning begins.

\paragraph{Benefits for this task.}
\begin{itemize}
    \item \textbf{Sharper localization:} Attention suppresses uniform background activation, isolating true edge and corner defects.
    \item \textbf{Improved centering awareness:} The spatial mask naturally measures left/right and top/bottom border differences, reinforcing centering cues.
    \item \textbf{Parameter efficiency:} CBAM adds only a few thousand parameters, negligible compared to the encoder, allowing fine spatial reasoning without overfitting.
    \item \textbf{Compatibility with residual learning:} The skip connections ensure that baseline edge detectors flow through unchanged, while attention selectively boosts or attenuates them.
\end{itemize}

In practice, the attention maps highlight the card perimeter and damaged zones, 
mirroring how a human grader’s gaze concentrates on corners and edges when determining PSA quality.


\subsection{Centering Sensitivity}
\label{subsec:center}

\paragraph{Task rationale.}
Centering---the symmetry of the printed border around the artwork---is a primary driver of PSA grade. 
Because all images come from the same scanner (fixed DPI, orientation, and field of view), absolute pixel coordinates carry genuine geometric meaning. 
We therefore preserve spatial layout and restrict random translation during augmentation to $\pm 1$--$2\%$, so asymmetries in left/right and top/bottom margins remain visible to the network.

\paragraph{How the encoder captures centering.}
Convolutions propagate small physical offsets as corresponding shifts in intermediate feature maps (i.e., they preserve spatial differences rather than erasing them). 
Consequently, if the left border is thicker than the right, channels that respond to border transitions activate more on one side than the other. 
The final global pooling aggregates these imbalances into summary statistics that the classifier can interpret as evidence of off-centering.

\paragraph{Defining centering targets.}
To encourage explicit geometric awareness, we form weak supervision targets from the scan itself. 
Let the outer card rectangle have left/right $x$-coordinates $x_{\text{outer,left}}$, $x_{\text{outer,right}}$ and the inner printed frame (border–art boundary) have $x_{\text{inner,left}}$, $x_{\text{inner,right}}$. 
Define horizontal border widths
\[
L \!=\! x_{\text{inner,left}} - x_{\text{outer,left}},\qquad
R \!=\! x_{\text{outer,right}} - x_{\text{inner,right}},
\]
and analogously vertical widths using $y$-coordinates:
\[
T \!=\! y_{\text{inner,top}} - y_{\text{outer,top}},\qquad
B \!=\! y_{\text{outer,bottom}} - y_{\text{inner,bottom}}.
\]
The normalized centering ratios are
\[
C_x \;=\; \frac{|L - R|}{L + R},\qquad
C_y \;=\; \frac{|T - B|}{T + B},
\]
so $C_x,C_y \in [0,1)$, with larger values indicating worse centering. 
Measuring $L,R,T,B$ \emph{relative to the card edges} (not the image boundary) makes these ratios robust to tiny scan shifts.

\paragraph{Auxiliary centering head.}
From the back-branch embedding $h_b \in \mathbb{R}^{d_b}$ (after pooling), a linear head predicts centering offsets
\[
(\hat{c}_x,\hat{c}_y) \;=\; W_c h_b + b_c,
\]
where $W_c \in \mathbb{R}^{2 \times d_b}$ and $b_c \in \mathbb{R}^{2}$ are trainable parameters. 
We train this head with a small regression loss
\[
L_{\text{center}} \;=\; (\hat{c}_x - C_x)^2 \;+\; (\hat{c}_y - C_y)^2,
\]
which nudges shared features to encode border symmetry explicitly. 
To avoid overfitting to sub-pixel jitter, we cap targets at a small maximum (e.g., $C_x,C_y \leftarrow \min(C_x,C_y,\,0.15)$) and keep translation jitter low.

\paragraph{Effect on the main task.}
This auxiliary objective improves interpretability (the model reports estimated centering) and strengthens grade prediction by coupling texture cues (whitening, corner wear) with geometric evidence (border imbalance). 
In the total loss, $L_{\text{center}}$ is added with a small weight $\beta_{\text{center}}$ (e.g., $0.05$--$0.2$), so it guides representation learning without dominating the classification objective.

\paragraph{Variable summary.}
$h_b$: back-encoder embedding; $W_c,b_c$: parameters of the centering head; 
$L,R,T,B$: pixel border widths measured between the outer card edge and the inner printed frame; 
$C_x,C_y$: normalized horizontal/vertical centering ratios (targets); 
$\hat{c}_x,\hat{c}_y$: predicted centering ratios; 
$L_{\text{center}}$: auxiliary regression loss; $\beta_{\text{center}}$: its weighting in the total loss.


\subsection{Feature Fusion and Classifier Head}
Embeddings from both branches are fused with asymmetric weighting to reflect their relative diagnostic value:
\[
z = [\,\lambda h_b;\,(1-\lambda)h_f\,], \qquad \lambda \in [0.6,0.8].
\]
The fused vector passes through two affine layers with ReLU activations and dropout regularization, producing logits $s \in \mathbb{R}^{10}$ and class probabilities
\[
p_k = \frac{e^{s_k}}{\sum_{j=1}^{10} e^{s_j}}.
\]
The Softmax function ensures $p_k \in [0,1]$ and $\sum_k p_k = 1$, yielding a valid probability distribution over PSA grades. 
An expected-grade score $\mathbb{E}[y] = \sum_k k\,p_k$ provides a smooth numeric estimate for borderline cases, while the auxiliary centering output refines the network’s geometric understanding.


\section{Loss Functions: Ordered, Imbalanced Labels}

\subsection{Weighted Cross-Entropy (Class Imbalance)}
The dataset is heavily skewed toward higher grades (9--10), so we weight the cross-entropy loss inversely to class frequency.
Class weights $w_k \propto 1/\text{freq}(k)^\gamma$, with $\gamma \in [0.5,1]$, downweight common grades and amplify rare ones (1--6):
\[
L_{\text{CE}} = -\sum_{k=1}^{10} w_k\, t_k \log p_k,
\]
where $t_k$ is the one-hot ground truth label and $p_k$ is the predicted probability for grade $k$.

\subsection{Earth Mover's Distance (Ordinal Consistency)}
PSA grades are ordinal---misclassifying a 9 as an 8 is far less severe than predicting a 4. 
To model this ordering, we measure the distance between cumulative distributions of predicted and true grades:
\[
F_p(k)=\sum_{j=1}^{k}p_j, \quad F_t(k)=\sum_{j=1}^{k}t_j,
\]
and define
\[
L_{\text{EMD}} = \frac{1}{10}\sum_{k=1}^{10}\big(F_p(k)-F_t(k)\big)^2.
\]
This encourages smooth probability flow between adjacent grades, enforcing ordinal coherence in the output distribution.

\subsection{Auxiliary Edge-Damage Objective}
Surface whitening and edge chipping are clear physical indicators of condition but not always labeled directly. 
We add a weak binary auxiliary objective that predicts whether edge damage is detected on the back:
\[
q = \sigma(u), \qquad 
L_{\text{edge}} = -\big[d\log q + (1-d)\log(1-q)\big],
\]
where $d \in \{0,1\}$ is a heuristic damage label estimated from border contrast and $u$ is the scalar logit from the auxiliary edge head.
This term guides early convolutional filters to encode defect-sensitive texture cues.

\subsection{Auxiliary Centering Objective}
Centering errors are geometric rather than textural, so we supervise them with a small regression loss on predicted centering offsets:
\[
(\hat{c}_x,\hat{c}_y) = W_c h_b + b_c,
\]
\[
L_{\text{center}} = (\hat{c}_x - C_x)^2 + (\hat{c}_y - C_y)^2,
\]
where $C_x,C_y$ are the measured centering ratios derived from the card borders.
This loss encourages the shared representation to encode border symmetry explicitly and improves interpretability by yielding numerical centering estimates.

\subsection{Total Loss}
The complete training objective combines classification, ordinal, and auxiliary terms:
\[
\boxed{
L = L_{\text{CE}}
   + \alpha\,L_{\text{EMD}}
   + \beta_{\text{edge}}\,L_{\text{edge}}
   + \beta_{\text{center}}\,L_{\text{center}}
   + \lambda_W \|\Theta\|_2^2
}
\]
with weighting coefficients $\alpha \!\in\! [0.5,1]$, $\beta_{\text{edge}} \!\ll\! 1$, and $\beta_{\text{center}} \!\ll\! 1$.
$L_{\text{CE}}$ anchors classification accuracy, $L_{\text{EMD}}$ enforces ordinal smoothness, 
and the auxiliary terms regularize the feature space toward physically meaningful dimensions---edge wear and centering---mirroring how human graders reason about card quality.


\section{Data Augmentation: Invariances to Preserve or Suppress}
\label{sec:augmentation}

\paragraph{Purpose (task-specific).}
Augmentation enlarges the effective training distribution by applying \emph{label-preserving} transformations to each scan.
Formally, instead of minimizing risk only over the empirical samples $\{(x_i,y_i)\}$, we minimize the expected loss over a family of transformations $\mathcal{T}$:
\[
\min_\Theta \ \frac{1}{N}\sum_{i=1}^N \ \mathbb{E}_{\tau\sim\mathcal{T}} \big[\, L\big(f_\Theta(\tau(x_i)),\, y_i\big) \,\big].
\]
Choosing $\mathcal{T}$ defines which nuisance variations the model should become \emph{invariant} (or robust) to, and which visual cues it should \emph{preserve}.
For PSA grading, we want robustness to capture artifacts (lighting, tiny framing shifts) while \emph{preserving} diagnostic signals: edge whitening, corner chips, surface scratches, and border symmetry (centering).

\paragraph{Key principles for this dataset.}
All scans come from the same flatbed scanner with consistent DPI and lighting; residual variation is small (minor crop shifts, slight glare). 
Therefore, we use \emph{light} geometric jitter (to avoid erasing centering) and \emph{targeted} photometric jitter (to suppress irrelevant appearance while protecting true defects).

\subsection{Geometric (Shared)}
\label{subsec:aug-geom}
We apply small affine and perspective jitters that mimic real capture variability while keeping border geometry intact:
\begin{itemize}
  \item \textbf{Rotation:} $\pm 2^\circ$ — covers tiny misalignments without introducing unrealistic tilt.
  \item \textbf{Translation/Scale:} $\pm 2\%$ — matches residual crop shifts from the PSA casing; preserves absolute layout used for centering.
  \item \textbf{Perspective:} $\le 0.02$ — accounts for minimal scanner glass/lens effects without distorting borders.
\end{itemize}
These transforms teach the model that \emph{where} a feature appears can wobble slightly (scanner jitter) yet true centering—relative border widths—remains meaningful.
We deliberately \emph{avoid} large translations or aggressive random cropping that would decouple pixel coordinates from border symmetry.

\subsection{Photometric (Branch-Specific)}
\label{subsec:aug-photo}
Augmentations differ for the front and back because their visual variability and diagnostic content differ.

\paragraph{Front (appearance invariance).}
Front images vary widely by set, holo pattern, ink density, and artwork color. 
We therefore encourage invariance to appearance while retaining structural cues (scratches, centering lines):
\begin{itemize}
  \item \textbf{Color and lighting jitter (strong):} perturb brightness/contrast/saturation/hue and color temperature to reduce reliance on artwork palette or scan exposure.
  \item \textbf{Mild blur \& compression:} small Gaussian blur and JPEG artifacts simulate common export noise while keeping linear scratches detectable.
  \item \textbf{Highlight synthesis:} add low-opacity specular blobs/strips to mimic holo glare so the network does not mistake glare for damage.
\end{itemize}
Effect: the front branch learns to \emph{ignore} decorative variability and transient glare, focusing on stable structure (edge transitions, large scratches, print box geometry).

\paragraph{Back (defect sensitivity).}
Back designs are standardized; subtle texture differences (whitening, chipping) dominate:
\begin{itemize}
  \item \textbf{Brightness/contrast jitter (mild):} normalizes residual exposure without washing out white specks.
  \item \textbf{Gaussian noise \& compression (mild):} reflects scanner/equipment noise, improving robustness to minor grain.
  \item \textbf{Low-opacity speckle synthesis (edge-focused):} rare, tiny white specks near the rim expand defect diversity while remaining realistic; used sparingly to avoid label drift.
\end{itemize}
Effect: the back branch becomes robust to benign capture noise yet \emph{more sensitive} to true wear patterns along borders and corners.

\subsection{Multi-Scale Crop Jitter}
\label{subsec:aug-crop}
We apply random-resized crops with scale in $[0.95,1.0]$ around the full card. 
This simulates small screenshot framing inconsistencies (some edges show a sliver of the PSA case) while \emph{keeping the entire border visible}.
By constraining the scale range tightly, we maintain the pixel-level relationship between inner print frame and outer card edge needed for centering.

\subsection{Why augmentation helps our use case}
\label{subsec:aug-benefits}
\begin{itemize}
  \item \textbf{Improved generalization with limited data:} With $\sim 10$k cards and a heavy grade skew, augmentation increases ``effective'' sample diversity, reducing overfitting to scanner quirks or particular artworks.
  \item \textbf{Explicit invariances:} Front augmentations teach invariance to colorway and glare; back augmentations teach robustness to benign noise while preserving edge/texture signals—mirroring how human graders discount lighting but not whitening.
  \item \textbf{Calibration under ordinal loss:} By exposing the model to mild perturbations near decision boundaries (e.g., slight glare vs. real scratch), the predicted distributions $p_k$ become smoother and more ordinally consistent (supporting EMD).
  \item \textbf{Centering reliability:} Tight geometric jitter (Sec.~\ref{subsec:aug-geom}) and crop constraints (Sec.~\ref{subsec:aug-crop}) protect absolute spatial cues so left/right and top/bottom border asymmetries remain learnable.
  \item \textbf{Imbalance-aware usage:} We may apply \emph{stronger} augmentations to majority grades (8--10) and \emph{lighter} ones to minority grades (1--6), increasing diversity where data are plentiful without corrupting scarce signals.
\end{itemize}

\paragraph{What we deliberately avoid.}
No heavy rotations, large random translations, extreme crops, or grayscale-only training, as these would either remove centering information or erase subtle whitening cues that drive the PSA grade.

\paragraph{Takeaway.}
Augmentation is not decoration; it encodes the invariances we want the model to learn and the sensitivities we want it to retain. 
In this task, that means: \emph{tolerate} small capture artifacts and artwork color variation, while \emph{preserving and amplifying} edge whitening, corner wear, surface scratches, and border symmetry.


\section{Training Curriculum and Data Splits}
\subsection{Splits}
The data is divided 70\% train, 15\% validation, 15\% test, stratified by grade and card identity to avoid leakage.

\subsection{Curriculum}
\begin{enumerate}
    \item \textbf{Back-only pretraining:} learn robust defect features.
    \item \textbf{Dual-branch fine-tuning:} add the front branch and fusion head, with reduced learning rate for the back encoder.
\end{enumerate}

\subsection{Imbalance-Aware Sampling}
Mini-batches sample grades with probability $\propto 1/\text{freq}(k)^\eta$ ($\eta\in[0.3,0.7]$), complementing the weighted loss.

\section{From Logits to Predictions and Uncertainty}
The model outputs $p\in\Delta^9$. 
We compute:
\[
\hat{k} = \arg\max_k p_k,\qquad 
\hat{g} = \sum_k k\,p_k,\qquad 
H(p) = -\sum_k p_k\log p_k.
\]
The mode gives the discrete grade, the expectation yields a continuous score, and entropy quantifies uncertainty.
Temperature scaling ($p_k \propto e^{s_k/T}$) can improve calibration.

\section{Why These Choices Reflect Real Grading and Sound Mathematics}
\begin{itemize}
    \item Residual CNNs capture local-to-global structure while avoiding vanishing gradients.
    \item Asymmetric depth mirrors human grading priorities: backs are consistent and diagnostically rich.
    \item Attention highlights edges and corners, echoing where graders look.
    \item EMD-based ordinal loss respects the metric nature of grades.
    \item Weighted losses and sampling address dataset imbalance.
    \item Augmentations explicitly encode desired invariances: robustness on fronts, sensitivity on backs.
\end{itemize}

\section{Evaluation Notes}
Performance should be reported via per-grade accuracy, mean absolute error on numeric grade, and confusion matrices showing near-diagonal errors (e.g., 9$\leftrightarrow$10). 
Grad-CAM visualizations are expected to highlight edges and corners on backs, and surface scratches or centering lines on fronts.

\section{Conclusion}
This architecture is not a generic image classifier retrofitted to grading; it is task-aligned at every level: geometry normalization, asymmetric residual encoders, edge-focused attention, and losses that understand that 10 is next to 9. 
The output is a calibrated probability distribution suitable for both automated decisions and human-in-the-loop review, faithfully encoding how expert graders evaluate cards.

\end{document}
